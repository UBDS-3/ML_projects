{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>MNIST Dataset</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most commonly used datasets for educational purposes is the MNIST dataset. You can download it from deep learning libraries such as keras and pytorch and it consists of 28x28 pixel greyscale images of handwritten digits. Each image has the corresponding label and it can be used to train and test neural network models for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.zeros((3,4))\n",
    "b = np.zeros((3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack((a,b),2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NLZathBN9php"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillemguigoicorominas/opt/anaconda3/envs/workspace/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use PyTorch to build out deep learning models. The following cells are for you to get familiar with how pytorch tensors work.\n",
    "\n",
    "*The following cells are just for you to execute and observe the result. Jump to the section* **Working with Fashion-MNIST** *for the explanation of the code.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FnAdkv7iO1X0"
   },
   "outputs": [],
   "source": [
    "# pytorch and numpy work well together\n",
    "some_numpy_array = np.array([1,2,3])\n",
    "some_torch_tensor = torch.tensor(some_numpy_array)\n",
    "some_numpy_array = some_torch_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddBnXjLvAdkH",
    "outputId": "24d61d3a-f4f0-48da-c674-76e685e25bd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_numpy_array.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_torch_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8YVOSRCMqj8",
    "outputId": "f4bb60e0-fb80-4dca-fcba-398a4e211e8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.9556, -0.8745, -0.1729, -1.6754, -0.2514,  2.7289,  0.7543,\n",
      "           1.6180, -0.2368,  1.0759,  1.7754,  2.0707, -0.0405,  0.2174,\n",
      "          -0.2416,  1.5176]]])\n",
      "torch.Size([1, 1, 16])\n"
     ]
    }
   ],
   "source": [
    "# define tensor of random numbers\n",
    "input_data = torch.randn(1,1,16)\n",
    "print(input_data)\n",
    "print(input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_1 = torch.ones((4,4))\n",
    "matrix_2 = torch.ones((4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.hstack((matrix_1, matrix_2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.vstack((matrix_1, matrix_2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dstack((matrix_1, matrix_2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dstack((matrix_1, matrix_2)).ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GTd-KHAOM4qS",
    "outputId": "24c2579a-8ac0-4ff1-ff13-0a6f10a01462"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[-0.0641, -0.1193,  0.2128,  0.0967,  0.0949,  0.2236,  0.0975,  0.1366,\n",
       "                       -0.1461,  0.0417, -0.1915, -0.0382,  0.2436,  0.0326,  0.1801,  0.1768],\n",
       "                      [-0.0898,  0.1683,  0.1672,  0.2459,  0.0456,  0.0933, -0.0588, -0.0522,\n",
       "                       -0.2043, -0.1723, -0.0819,  0.2251,  0.1659,  0.1871, -0.2073,  0.0096],\n",
       "                      [ 0.2062,  0.1518, -0.2454, -0.0708,  0.2373, -0.2323, -0.0026, -0.1778,\n",
       "                        0.2220, -0.0834, -0.0674,  0.0488,  0.1159, -0.2165, -0.2328, -0.2049],\n",
       "                      [ 0.0134, -0.0971, -0.2213, -0.0780, -0.1399, -0.1320, -0.0127,  0.0945,\n",
       "                       -0.0829, -0.1397, -0.0744, -0.1428,  0.0624, -0.0375, -0.0675, -0.0622],\n",
       "                      [-0.1736,  0.1409, -0.1115,  0.2349,  0.0693, -0.1960,  0.2180, -0.0740,\n",
       "                        0.0016,  0.2177,  0.0333,  0.0662,  0.0642, -0.1858, -0.1004,  0.0258],\n",
       "                      [ 0.2062, -0.0454,  0.0374, -0.2065,  0.1901,  0.1865, -0.0223,  0.1347,\n",
       "                        0.0302, -0.1390, -0.1299,  0.0099,  0.2430, -0.0189,  0.1954,  0.1771],\n",
       "                      [ 0.0454,  0.1375,  0.0634,  0.0876,  0.0747, -0.2150, -0.2079,  0.1102,\n",
       "                        0.0067, -0.1785, -0.1147, -0.1364, -0.0742, -0.2200,  0.0307,  0.1923],\n",
       "                      [-0.1315,  0.0068, -0.1001, -0.1855,  0.0336, -0.1520,  0.1087, -0.2175,\n",
       "                        0.0208, -0.0635,  0.1715,  0.2243,  0.0235, -0.1267,  0.1841,  0.1201]])),\n",
       "             ('bias',\n",
       "              tensor([-0.0361, -0.0103, -0.0767, -0.0859, -0.1203, -0.2146,  0.2182,  0.1475]))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define linear layer\n",
    "linear = nn.Linear(16, 8)\n",
    "linear.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_data_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m linear(\u001b[43minput_data_1\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_data_1' is not defined"
     ]
    }
   ],
   "source": [
    "linear(input_data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(input_data_1, linear.state_dict()['weight'].T) + linear.state_dict()['bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(linear.state_dict()['weight'][0] * input_data) + 0.2358"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(input_data, linear.state_dict()['weight'].T) + linear.state_dict()['bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "slgJLHZiOOyL",
    "outputId": "c33c3e6f-b304-4f95-b83c-32180e74b796"
   },
   "outputs": [],
   "source": [
    "output = linear(input_data)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RvQc8Z7rB4dT",
    "outputId": "0efbe844-db22-44f7-9c47-26fc9b7f75bd"
   },
   "outputs": [],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "npITLguRB3Ab",
    "outputId": "52bc4c40-90e7-4c1e-a6c6-90121d250285"
   },
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ySThG-I1OsWR",
    "outputId": "1d9104ad-4e7b-4ed1-ff9c-f5a5972b87ee"
   },
   "outputs": [],
   "source": [
    "# obtain the same output using matrix multiplication (torch.matmul(t1, t2))\n",
    "torch.matmul(input_data, linear.state_dict()['weight'].T) + linear.state_dict()['bias']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Working with Fashion-MNIST</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST is the \"hello, world!\" of datasets for image classification. Since it is arguably overused, today we will download the Fashion-MNIST dataset. As explained by the authors:\n",
    "\n",
    "\"Fashion-MNIST is a dataset of Zalando's article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. We intend Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\"\n",
    "\n",
    "You can find the original repository [here](https://github.com/zalandoresearch/fashion-mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jOs6qq-_Mkm7"
   },
   "outputs": [],
   "source": [
    "def transform_labels(label):\n",
    "    label_vector = torch.zeros(10)\n",
    "    label_vector[label] = 1\n",
    "    return label_vector\n",
    "\n",
    "train_dataset = datasets.FashionMNIST('./fmnist_data', download=True, train=True, transform=transforms.Compose([\n",
    "                                                transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                                transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "                                                ]), target_transform=transform_labels)\n",
    "\n",
    "test_dataset = datasets.FashionMNIST('./fmnist_data', download=True, train=False, transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                                              transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "                                                          ]), target_transform=transform_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time you execute this code PyTorch will download the dataset and store it in the specified folder (./fmnist_data). You can load it afterwards at any time without having to download it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./fmnist_data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "           )\n",
       "Target transform: <function transform_labels at 0x7fc1fd25f670>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: ./fmnist_data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "           )\n",
       "Target transform: <function transform_labels at 0x7fc1fd25f670>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digital images consist of pixels that (typically) range from 0 to 255 in value, consisting of an 8-bit representation of color ($2^8=256$ values). Most digital images are RGB, which combine different values for Red, Green and Blue pixels between 0 and 255. You can imagine this as three images superimposed on top of eachother that combined make the colors of the visible spectrum. These are called *channels*, and a typical RGB image has 3. The images in the current dataset have a single channel, as they are grayscale. The pixel values in the raw images range from 0 to 255, but here we already normalized the data with mean 0.1307 and standard deviation 0.3081 in the loading step. These are arbitrary values taken from PyTorch's own tutorial on how to work with the Fashion-MNIST dataset (https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n",
    "\n",
    "Each element in the train and test datasets is a tuple that contains both the image in tensor form and the label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc1e52ea370>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.numpy()[0], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 28, 28])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(type(image))\n",
    "print(image.shape)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of [1, 28, 28] specifies that the image has a height and width of 28 pixels and a single channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch's tutorial page we can find the following labels map, which tells us the classes that correspond to each numeric label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'T-Shirt'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_map.get(torch.argmax(label).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the first image in the training set is an Ankle Boot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.02, 'T-Shirt')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.squeeze(), cmap='gray')\n",
    "plt.title(labels_map.get(torch.argmax(label).item()), fontsize=14, y=1.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the data in the train and test datasets tells us that there are 60.000 images in the train set and 10.000 images in the test set.\n",
    "\n",
    "We will use PyTorch's DataLoader to iterate over the samples in the training and test sets. Here is where we will select the batch size, i.e. the number of samples that we will use as input to the model at once. The batch size is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "3QuJIOzePM8s"
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to define our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.6899,  0.4674, -0.8456, -1.5089, -1.7153,  0.3057, -0.3905,\n",
      "           1.1778,  1.4188, -0.0906]]])\n",
      "tensor([[[0.0000, 0.4674, 0.0000, 0.0000, 0.0000, 0.3057, 0.0000, 1.1778,\n",
      "          1.4188, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "dropout = nn.Dropout(0.6)\n",
    "random_tensor = torch.randn(1,1,10)\n",
    "print(random_tensor)\n",
    "print(F.relu(random_tensor))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "VgPtwoVVQVSL"
   },
   "outputs": [],
   "source": [
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self, use_dropout: bool = True) -> None:\n",
    "        super(FullyConnected, self).__init__()\n",
    "        self.linear_1 = nn.Linear(784, 64)\n",
    "        self.linear_2 = nn.Linear(64, 32)\n",
    "        self.output = nn.Linear(32, 10)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.use_dropout = use_dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear_1(x))\n",
    "        if self.use_dropout:\n",
    "            x = self.dropout(x)\n",
    "        x = F.relu(self.linear_2(x))\n",
    "        x = self.softmax(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "iDDq_Q4ERh_9"
   },
   "outputs": [],
   "source": [
    "# here we specify that we want to use the GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = FullyConnected().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "jlmYAqqETXUc"
   },
   "outputs": [],
   "source": [
    "# define optimizer\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we move onto the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, training loss: 0.7443296682026427, test loss: 0.5129877403378487\n",
      "epoch: 1, training accuracy: 0.7581333333333333, test accuracy: 0.8161 \n",
      "\n",
      "epoch: 2, training loss: 0.4457270828849178, test loss: 0.44353802800178527\n",
      "epoch: 2, training accuracy: 0.8440833333333333, test accuracy: 0.8428 \n",
      "\n",
      "epoch: 3, training loss: 0.3995730922383777, test loss: 0.42952619194984437\n",
      "epoch: 3, training accuracy: 0.8585333333333334, test accuracy: 0.8454 \n",
      "\n",
      "epoch: 4, training loss: 0.37231819306389763, test loss: 0.40394589900970457\n",
      "epoch: 4, training accuracy: 0.8681666666666666, test accuracy: 0.8559 \n",
      "\n",
      "epoch: 5, training loss: 0.35277937630475575, test loss: 0.39405733048915864\n",
      "epoch: 5, training accuracy: 0.87405, test accuracy: 0.8569 \n",
      "\n",
      "epoch: 6, training loss: 0.3394458192889973, test loss: 0.382901431620121\n",
      "epoch: 6, training accuracy: 0.87805, test accuracy: 0.8636 \n",
      "\n",
      "epoch: 7, training loss: 0.32897387040873705, test loss: 0.37257121652364733\n",
      "epoch: 7, training accuracy: 0.8812, test accuracy: 0.8666 \n",
      "\n",
      "epoch: 8, training loss: 0.3138749947992422, test loss: 0.36538894325494764\n",
      "epoch: 8, training accuracy: 0.8876833333333334, test accuracy: 0.8675 \n",
      "\n",
      "epoch: 9, training loss: 0.3049380236257941, test loss: 0.35669270604848863\n",
      "epoch: 9, training accuracy: 0.8908333333333334, test accuracy: 0.8727 \n",
      "\n",
      "epoch: 10, training loss: 0.29686323502811335, test loss: 0.3548991471529007\n",
      "epoch: 10, training accuracy: 0.8929, test accuracy: 0.8733 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "epochs = 10\n",
    "train_loss, test_loss = [], []\n",
    "train_accuracy, test_accuracy = [], []\n",
    "best_model = deepcopy(model)\n",
    "best_test_loss = torch.inf\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_epoch_train, loss_epoch_test = 0, 0\n",
    "    for i, (img, labels) in enumerate(train_loader):\n",
    "        input_data = img.to(device).reshape(-1,784)\n",
    "        labels = labels.to(device)\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(input_data)\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # Add loss values for this epoch\n",
    "        loss_epoch_train += loss.item()\n",
    "        for im, lab in zip(outputs, labels):\n",
    "            if im.argmax() == lab.argmax():\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    accuracy_train_epoch = correct / total\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for j, (img, labels) in enumerate(test_loader):\n",
    "        input_data = img.to(device).reshape(-1,784)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(input_data)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss_epoch_test += loss.item()\n",
    "        for im, lab in zip(outputs, labels):\n",
    "            if im.argmax() == lab.argmax():\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        accuracy_test_epoch = correct / total\n",
    "        if loss_epoch_test < best_test_loss:\n",
    "            best_test_loss = loss_epoch_test\n",
    "            best_model = deepcopy(model)\n",
    "    print(f'epoch: {epoch + 1}, training loss: {loss_epoch_train / (i+1)}, test loss: {loss_epoch_test / (j+1)}')\n",
    "    print(f'epoch: {epoch + 1}, training accuracy: {accuracy_train_epoch}, test accuracy: {accuracy_test_epoch} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.02, 'Coat')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAp8AAAFFCAYAAACqpYbcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAfcElEQVR4nO3df3Bd9Xnn8c9jWb8sCfmXEMKxIXYN1FBwiJfstrCl46UDaQPJ7jotnXboTGac2QkzyW53t9lkdkM6k002S8IymzbEAQY6S5Jmxk7hD0phKWlKBrMxP4oxDj9C7ICx/CPGWJKxfvnZP+7xVNdYPo90z/3ee6T3a0Yj6dxH3/M9OtePH5177vcxdxcAAACQwoJGTwAAAADzB8UnAAAAkqH4BAAAQDIUnwAAAEiG4hMAAADJUHwCAAAgGYpPJGVme8zsP9YaAwAAyoniE4Uysz4z+4usgBw1swNm9riZXTeDYf6ZpL/I2c+1ZuZmtry2GQMAzKzfzO40s59luXufmf2NmX24wH1wYQGSpIWNngDmnK2SFkn6hKTXJJ0r6TclLYsO4O6Hzva4mbXVMkEAwD8xswsl/VjSkKT/IukfVbk4tVHSXZJWNWxymJO48onCmNliSddI+qy7P+7ue939J+5+u7t/b0poh5l9y8yOmdmbZvafThun6q/j7Arnp8xsm5mNSPqOpCeyhw9lj99X36MDgDnr1CtNG9z9++7+srvvdvdvSLpcksxslZn9wMyGso9tZva+UwOY2Roze9DMBs1sxMyeNbPfnfL4DyVdIOl/Zjmb9orzGMUnijScfdxoZh1nifv3knZKulLS/5D0VTP7Fzljf0HSw5J+TdKfSvo32fZLJQ1I+nQN8waAecnMlkq6XtKfu/vw6Y+7+1EzWyDpQUn9kn4r+zhf0l+bmWWh3ZL+RtJ1kq5Q5VWwbWZ2Sfb4v5b0pqQ/UyVnD9TtoND0KD5RGHefkPTHkv5Q0lEze8rMbjezD50W+qi7f8PdX3P3/63Ky/Mbc4b/K3e/291fd/efSTqSbT/o7oPu/k6RxwIA88SvSDJJu88Ss1GVK6B/4O473H2HpD9Q5QLCRkly939097vcfWeW278k6VlJ/zZ7/IikSUlDWc4erN8hodlRfKJQ7r5Vlb+IP6LKX8G/Lmm7mX1uStgLp/3YW6rcG3o2OwqbJADgFMsP0a9Kesvd95za4O6vq5K710mSmXWZ2VfN7CUze9vMhiVtEPeL4gwoPlE4dz/h7o+5+5+5+69LukfSbVPeKDR++o8o/7k4UvQ8AQB6VZUc/Kuz/PlT927eLmmTpP+qyptM10v6f5J4gyjeg+ITKbykysoKZ7sPdKbGss8tBY4JAPNK9nL430q61cy6T388eyPpbknnZ++KP7V9tSqvcr2Ubbpa0l+6+1Z3f0GV+zvXnDbcmMjZEMUnCmRmy8zs78zsD83scjN7v5ltkvSfJT3u7scK3N1eVf7i/p1sbdH3JE0AQMinVHn5fYeZbTKzi83sEjP7d6rcJvV/s88PmNkGM9sg6QFV7un8u2yMVyR9zMyuNLNfk/R/9N4LDnskXWNmK1ijeX6j+ESRhiVtV+Wd538vaZek/67K0ki/V+SO3H2fKu+A/5KkA5K+UeT4ADBfZPdvXinpMVVWIHlBlaLyRkmb3d0l3STpkCrL3D0haVDSR7PHJOk/SDoo6R9Uud9/e/b1VP9N0kpJP8vGwjxl//S8AQAAAOqLK58AAABIhuITAAAAyVB8AgAAIBmKTwAAACRD8QkAAIBkKD4BAACQDMUnAAAAkqH4BAAAQDIUnwAAAEiG4hMAAADJLEy5MzOjl+csmFluTFtbW2isjo6OUNzIyEgobmJiIhTXrIr+vR07dqyW6eDsDrt7X6MnMZ+Qs2eHnF0/5OxSmTZn11R8mtn1ku6U1CLpbnf/Si3j1cOCBbGLuydPnixsvOhYUa2trbkxK1euDI116aWXhuKefvrpUNzg4GAorlkNDAyE4tatWxeKe+SRR3Jj3NP/f170v4MG2dvoCcwFzZ63ydnVyNnVyNnVypqzZ/2yu5m1SPpzSTdIWifpZjOLnW0AQHLkbQDNoJZ7Pq+S9Jq7v+7uY5K+J+mmYqYFAKgD8jaAhqul+Fwh6Y0p37+ZbQMANCfyNoCGq/sbjsxss6TN9d4PAKB25GwA9VZL8blP0tQ7pt+Xbavi7lskbZF45yQANFhu3iZnA6i3Wl52/4mktWb2fjNrk/T7kh4qZloAgDogbwNouFlf+XT3CTO7VdLfqrJkx73uvquwmQEACkXeBtAMarrn090flvRwQXMBANQZeRtAo1nKxVMbcf9QpNPETOKKXND1W9/6Viiuvb09N2Z0dDQ0Vn9/fyiup6cnFBd5/kQ7Ujz33HOhuM7OztyY8fHx0FjRBZyHhoZCca+//npuzOLFi0NjPfRQ7NXQrVu3huIimnxh42fcfUMjdjxfkbOrkbOrkbOrkbPfY9qcTW93AAAAJEPxCQAAgGQoPgEAAJAMxScAAACSofgEAABAMhSfAAAASIbiEwAAAMlQfAIAACCZOb/IfCMWYP3yl78ciluzZk0o7q233sqNiS4KPDk5GYrr7e0NxQ0MDOTGbNu2LTTWXXfdFYp76qmncmMOHDgQGmtkZCQUd/jw4VBcS0tLbkz0Obl06dJQ3Pbt23Nj7rjjjtBYkflL8edRwVhkPjFydjVydjVydjVy9nuwyDwAAAAaj+ITAAAAyVB8AgAAIBmKTwAAACRD8QkAAIBkKD4BAACQDMUnAAAAkqH4BAAAQDIUnwAAAEhmYaMnUG9Fd8tYvXp1bsxll10WGusXv/hFKK69vT03JtqpKnqc+/btC8VF5nbBBReExtq0aVMo7vjx47kxhw4dCo01NDQUiot2kYj8fqOdJiJdUqTY863oLhhN3lUDJUbOrkbOrkbOrlbWnM2VTwAAACRD8QkAAIBkKD4BAACQDMUnAAAAkqH4BAAAQDIUnwAAAEiG4hMAAADJUHwCAAAgGYpPAAAAJDPnOxxNTEwUOt7GjRtzY6IdKbq6ukJxJ06cyI1ZuLDYU9nd3R2K279/f27M8uXLQ2N95CMfCcU999xzuTE9PT2hsTo7O0Nx0XM6Pj6eGxPt4GJmobi2trbcmGuuuSY01g9/+MNQXHRuwEyRs2eHnF2NnF2t2XI2Vz4BAACQDMUnAAAAkqH4BAAAQDIUnwAAAEiG4hMAAADJUHwCAAAgGYpPAAAAJEPxCQAAgGQoPgEAAJDMnO9wVLR169blxkQ7CUS7ZYyNjRW2T3cPxUW7Q7S2tubGjI6OhsYaGRkJxUW6Q0T3GZm/JE1OTobiIp1Nent7Q2N1dHSE4iLn9LLLLguNFe2WUXQXGqBeyNnVyNnVyNmNUVPxaWZ7JA1JmpQ04e4bipgUAKA+yNsAGq2IK5+/5e6HCxgHAJAGeRtAw3DPJwAAAJKptfh0SY+a2TNmtvlMAWa22cx2mNmOGvcFAKjdWfM2ORtAvdX6svvV7r7PzM6V9JiZ/dTdfzQ1wN23SNoiSWYWu3MaAFAvZ83b5GwA9VbTlU9335d9PijpB5KuKmJSAID6IG8DaLRZF59m1mVmPae+lvTbkl4samIAgGKRtwE0g1pedu+X9INsrbKFkr7j7o8UMisAQD2QtwE03KyLT3d/XdIVBc6lFNasWZMbE13MNbpYbmdnZ25MZKFcSRofHw/FRRfojSyU3NLSEhorOrfIgsXR+UfPVTSuvb09Nya6GHTkvEuxc9DX1xcaC3PbfMzb5Oxq5Oxq5OzGYKklAAAAJEPxCQAAgGQoPgEAAJAMxScAAACSofgEAABAMhSfAAAASIbiEwAAAMlQfAIAACAZik8AAAAkU0t7zTkl2rlieHg4N6anpyc0VrQ7xIoVK3Jj3njjjdBY0a4aCxbE/i6JdsKIiHSaiIp01JDinSuKFD3OpUuXhuIi53716tWhsYCyIGdXI2fXDzm7eFz5BAAAQDIUnwAAAEiG4hMAAADJUHwCAAAgGYpPAAAAJEPxCQAAgGQoPgEAAJAMxScAAACSofgEAABAMnQ4ygwMDITiFi1alBvj7qGxuru7Q3GRrgkvv/xyaKxoF4wiu2VEO1JEO29Efr9mFhorKnoMo6OjuTFXXnllaKyRkZFQXKTTy+LFi0NjAWVBzp5dHDm7Gjm7MbjyCQAAgGQoPgEAAJAMxScAAACSofgEAABAMhSfAAAASIbiEwAAAMlQfAIAACAZik8AAAAkwyLzmegispHFYaOL5XZ2dobiIgv5TkxMhMaKzF+KL9AbiYsu4Fyk6D6jxxk9p5OTk7kx0XPV29sbihscHMyN+eUvfxka68ILLwzF7dmzJxQH1As5uxo5uxo5u1qz5WyufAIAACAZik8AAAAkQ/EJAACAZCg+AQAAkAzFJwAAAJKh+AQAAEAyFJ8AAABIhuITAAAAyVB8AgAAIBk6HGX6+/tDcZGuCaOjo6GxzjvvvFDcsWPHcmOiXTDGx8dDcZEOHVLs97FgQexvnGiHi0hHiuhY0d9HtFtG5DxEnx+rV68Oxb3yyiu5MdH5r1+/PhTXbN0yMP+Qs6uRs6uRs6s1W87myicAAACSyS0+zexeMztoZi9O2bbUzB4zs1ezz0vqO00AQBR5G0Azi1z5vE/S9adt+6ykx919raTHs+8BAM3hPpG3ATSp3OLT3X8k6chpm2+SdH/29f2SPlrstAAAs0XeBtDMZvuGo3533599PShp2ju/zWyzpM2z3A8AoBihvE3OBlBvNb/b3d3dzKZ9m5q7b5G0RZLOFgcASONseZucDaDeZvtu9wNmNiBJ2eeDxU0JAFAH5G0ATWG2xedDkm7Jvr5F0oPFTAcAUCfkbQBNIbLU0nclPSXpYjN708w+Iekrkq4zs1cl/avsewBAEyBvA2hmufd8uvvN0zy0seC5NNSaNWtCcZFuCCdOnAiNtWzZslBcpBvCyZMnQ2NFu2pERTphRDtXFNmhIyr6e4vObXh4uLCxonGR32+0Y8nFF18cikNzmw95m5w9O+TsauTsxqDDEQAAAJKh+AQAAEAyFJ8AAABIhuITAAAAyVB8AgAAIBmKTwAAACRD8QkAAIBkKD4BAACQDMUnAAAAksntcDRfDAwMhOI6OjpyY44ePRoaa9GiRaG4SPeNhQtjpzLauSIq2oUhItoFY2JiorB9Ro2Ojobi2tracmPefvvt0FjRziaRc9DV1RUaK/rvAGg0cvbskLOrkbMbgyufAAAASIbiEwAAAMlQfAIAACAZik8AAAAkQ/EJAACAZCg+AQAAkAzFJwAAAJKh+AQAAEAyLDKfWbZsWSguuohsRHTh3XfffbewfUYXGC46LiK6YHHkHJw8eTI0VmQxaCm2ELEUWxB6eHg4NFZU5PdxzjnnhMY6//zza50OkAQ5u75xEeTs2SFnc+UTAAAACVF8AgAAIBmKTwAAACRD8QkAAIBkKD4BAACQDMUnAAAAkqH4BAAAQDIUnwAAAEiG4hMAAADJ0OEo09nZGYqLdHTo6OgIjbV8+fJQ3MjISG5MS0tLaKyiRbpSRDtqRI8h2uGiyH1Gu29EOlccP348NNbY2FgoLvJ8i3b7KLL7CVBP5OzZIWdXI2c3RjlnDQAAgFKi+AQAAEAyFJ8AAABIhuITAAAAyVB8AgAAIBmKTwAAACRD8QkAAIBkKD4BAACQDMUnAAAAkpnzHY7a29sLHS/SmaCvry801vPPPx+KO3r0aG5Mf39/aKzR0dFQXLQ7RKTbRLQDw/j4eChu4cLinrbvvvtuofuMPN8OHDgQGivSJUWKdV2JdHmRpMnJyVBcpCuIFD+nwCnk7Grk7Grk7Gplzdm5zzAzu9fMDprZi1O23WZm+8zs+ezjw/WdJgAgirwNoJlF/ry5T9L1Z9h+h7uvzz4eLnZaAIAa3CfyNoAmlVt8uvuPJB1JMBcAQAHI2wCaWS1vOLrVzF7IXt5ZMl2QmW02sx1mtqOGfQEAapebt8nZAOpttsXnNyWtkbRe0n5JX5su0N23uPsGd98wy30BAGoXytvkbAD1Nqvi090PuPuku5+U9G1JVxU7LQBAkcjbAJrFrIpPMxuY8u3HJL04XSwAoPHI2wCaRe5CWGb2XUnXSlpuZm9K+oKka81svSSXtEfSJ+s3RQDATJC3ATSz3OLT3W8+w+Z76jCXuliyZNr3Qs1KZPHdnp6e0FjRRYGLXKA3uiCtu4fiIgvhRhfLLVL0OKOLKbe1tYXiIgtCd3V1hcaKLlh80UUX5cZEF8eOHue5554bitu3b18oDsUqc94mZ1cjZ1cjZ1cra86mvSYAAACSofgEAABAMhSfAAAASIbiEwAAAMlQfAIAACAZik8AAAAkQ/EJAACAZCg+AQAAkAzFJwAAAJIprg1Dk1q8eHEorr29PRQX6a4Q7Yawd+/eUFxHR0duzMTERGislpaWUFy0k0ekq0a0I0W0Q0dkvOj8o6JzizyPIh01JGnXrl2huFWrVuXGjI2NhcaKPj+iz3FgpsjZ1cjZs0POrtZsOZsrnwAAAEiG4hMAAADJUHwCAAAgGYpPAAAAJEPxCQAAgGQoPgEAAJAMxScAAACSofgEAABAMhSfAAAASGbOdzhqa2sLxY2Pj4fiIl0Cop03HnnkkVDcFVdckRsTnX+0c0XUwoX5T6FItw8p3tEhss+iu4JEjyFyHqLPj1dffTUUt2nTptyY7u7u0FjRc7Bo0aJQHDBT5Oxq5Oxq5OxqZc3ZXPkEAABAMhSfAAAASIbiEwAAAMlQfAIAACAZik8AAAAkQ/EJAACAZCg+AQAAkAzFJwAAAJKh+AQAAEAyc77D0cTERKHjmVlh+yyyO8SRI0dCY0W7ZUS7SETmNjIyEhprcnIyFBfpNhHtSBF1+PDhUJy758asXLkyNNaTTz4ZinvnnXdyY1pbW0NjDQ8Ph+J6e3tDccBMkbOrkbNnh5xdrdlyNlc+AQAAkAzFJwAAAJKh+AQAAEAyFJ8AAABIhuITAAAAyVB8AgAAIBmKTwAAACRD8QkAAIBk5vwi84sWLQrFjY+Ph+JOnDiRGxNdLDcyliS1tbXlxpx33nmhsaILG3d2dobili1blhtz8ODB0FhLliwJxUXO1dDQUGisyPwladWqVaG4o0eP5sZ0dXWFxoosfizFzv3OnTtDY0Wfk9HnBzBT5Oxq5Oxq5OxqZc3ZXPkEAABAMrnFp5mtNLMnzOwlM9tlZp/Oti81s8fM7NXsc+xPIABA3ZCzATS7yJXPCUl/4u7rJP1zSZ8ys3WSPivpcXdfK+nx7HsAQGORswE0tdzi0933u/uz2ddDknZLWiHpJkn3Z2H3S/poneYIAAgiZwNodjN6w5GZXSjpA5KeltTv7vuzhwYl9U/zM5slba5hjgCAWSBnA2hG4TccmVm3pK2SPuPux6Y+5pW3eJ3xbV7uvsXdN7j7hppmCgAII2cDaFah4tPMWlVJYg+4+7Zs8wEzG8geH5AUW5sBAFBX5GwAzSzybneTdI+k3e7+9SkPPSTpluzrWyQ9WPz0AAAzQc4G0Owi93z+hqQ/krTTzJ7Ptn1O0lckfd/MPiFpr6SP12WGAICZIGcDaGq5xae7PynJpnl4Y7HTKV6k04QU75rQ29ubGzM5ORkaq6enJxQX6ZoQ7dAxMTERiot2D2ltbc2N6evrC411ySWXhOK2b9+eG1N0h44FC2K3R0fOabQLxuDgYChu//79uTE//elPQ2OtXbs2FBf9d4X0yNnVyNnVyNnVyNmNQYcjAAAAJEPxCQAAgGQoPgEAAJAMxScAAACSofgEAABAMhSfAAAASIbiEwAAAMlQfAIAACAZik8AAAAkE2mvWWrd3d2FxkVEOkhI0oc+9KFQ3KFDh3JjVq5cGRprbGwsFNfV1RWKi3QGaWlpCY11+PDhUNzw8HBuTPR8LlwY+ydw5MiRUNyll16aG3P06NHQWNddd10oLtIpJdoVZHR0NBTX398figNmipxdjZxdjZxdraw5myufAAAASIbiEwAAAMlQfAIAACAZik8AAAAkQ/EJAACAZCg+AQAAkAzFJwAAAJKh+AQAAEAyc36R+b6+vlDca6+9Forr7e3Njenp6QmNNTg4GIrr6OjIjYkuNNvZ2RmKm5iYCMWZWW5MZP5SbCFiKbZIcnTR6OhxvvPOO6G4yELJ0XPV1tYWihsZGcmNueSSS0JjRX8f7h6KA2aKnF2NnF2NnF2trDmbK58AAABIhuITAAAAyVB8AgAAIBmKTwAAACRD8QkAAIBkKD4BAACQDMUnAAAAkqH4BAAAQDIUnwAAAEhmznc4inYciMaNjY3lxkS7Q0Q7Dhw/fjw3pr29PTTWiRMnQnFFWrx4cSju5z//eWH7jHTxkGK/W0lqaWkJxR08eDA3Jnreo91DhoaGcmMiHUakeCePaFcNYKbI2dXI2dXI2dXKmrO58gkAAIBkKD4BAACQDMUnAAAAkqH4BAAAQDIUnwAAAEiG4hMAAADJUHwCAAAgGYpPAAAAJEPxCQAAgGTmfIejaDeEc845JxS3Z8+e3Jje3t7QWH19faG47u7u3JiRkZFC9xntrhDZb7TrQ7TjR2dnZyguInreo/uMdMKIdstYtWpVKC7SuWJ8fDw01ttvvx2KK7KzCTAVOXt2+yRnz26f5OzGyL3yaWYrzewJM3vJzHaZ2aez7beZ2T4zez77+HD9pwsAOBtyNoBmF7nyOSHpT9z9WTPrkfSMmT2WPXaHu99ev+kBAGaInA2gqeUWn+6+X9L+7OshM9staUW9JwYAmDlyNoBmN6M3HJnZhZI+IOnpbNOtZvaCmd1rZkuKnhwAYPbI2QCaUbj4NLNuSVslfcbdj0n6pqQ1ktar8lf216b5uc1mtsPMdtQ+XQBABDkbQLMKFZ9m1qpKEnvA3bdJkrsfcPdJdz8p6duSrjrTz7r7Fnff4O4bipo0AGB65GwAzSzybneTdI+k3e7+9SnbB6aEfUzSi8VPDwAwE+RsAM0u8m7335D0R5J2mtnz2bbPSbrZzNZLckl7JH2yDvMDAMwMORtAU4u82/1JSXaGhx4ufjoAgFqQswE0uznf4WjXrl2huGhXjcsvvzw35vOf/3xorEiXA0latmxZbszhw4dDY0W7PqxduzYUd+ONN+bGRDqMSNLJkydDcRdddFFuzJEjR0Jjtba2huIeffTRUNyCBfm3UUe7qUTPaWS8D37wg6Gxjh49Gor78Y9/HIoDZoqcXY2cXY2cXa2sOZve7gAAAEiG4hMAAADJUHwCAAAgGYpPAAAAJEPxCQAAgGQoPgEAAJAMxScAAACSofgEAABAMubu6XZmlm5nM3TDDTeE4q6++urcmC9+8YuhscbGxkJxwNlEFiy+8847Q2M9+eSTobi77747FFewZ9x9QyN2PF+Rs6uRs1EEcjZXPgEAAJAQxScAAACSofgEAABAMhSfAAAASIbiEwAAAMlQfAIAACAZik8AAAAkQ/EJAACAZCg+AQAAkEzqDkeHJO09bfNySYeTTaJ4ZZ+/VP5jKPv8pfIfQ4r5X+DufXXeB6aYozlbKv8xlH3+UvmPoezzl+p/DNPm7KTF5xknYLajzC3zyj5/qfzHUPb5S+U/hrLPH3Fz4VyX/RjKPn+p/MdQ9vlLjT0GXnYHAABAMhSfAAAASKYZis8tjZ5Ajco+f6n8x1D2+UvlP4ayzx9xc+Fcl/0Yyj5/qfzHUPb5Sw08hobf8wkAAID5oxmufAIAAGCeaFjxaWbXm9nLZvaamX22UfOohZntMbOdZva8me1o9HwizOxeMztoZi9O2bbUzB4zs1ezz0saOcezmWb+t5nZvuw8PG9mH27kHM/GzFaa2RNm9pKZ7TKzT2fby3QOpjuG0pwHzE7Z8zY5O72y52yp/Hm7GXN2Q152N7MWSa9Iuk7Sm5J+Iulmd38p+WRqYGZ7JG1w99Ks9WVm/1LSsKS/dPfLsm1flXTE3b+S/YeyxN3/tJHznM40879N0rC7397IuUWY2YCkAXd/1sx6JD0j6aOS/ljlOQfTHcPHVZLzgJmbC3mbnJ1e2XO2VP683Yw5u1FXPq+S9Jq7v+7uY5K+J+mmBs1lXnH3H0k6ctrmmyTdn319vypPyqY0zfxLw933u/uz2ddDknZLWqFynYPpjgFzG3m7AcjZjVf2vN2MObtRxecKSW9M+f5NlfM/L5f0qJk9Y2abGz2ZGvS7+/7s60FJ/Y2czCzdamYvZC/xNOVLH6czswslfUDS0yrpOTjtGKQSngeEzYW8Tc5uHqXMFWXP282Ss3nDUW2udvcrJd0g6VPZywul5pX7MMq2BMI3Ja2RtF7Sfklfa+hsAsysW9JWSZ9x92NTHyvLOTjDMZTuPGDeIWc3h1LmirLn7WbK2Y0qPvdJWjnl+/dl20rF3fdlnw9K+oEqL0uV0YHsnpBT94YcbPB8ZsTdD7j7pLuflPRtNfl5MLNWVRLAA+6+LdtcqnNwpmMo23nAjJU+b5Ozm0MZc0XZ83az5exGFZ8/kbTWzN5vZm2Sfl/SQw2ay6yYWVd2467MrEvSb0t68ew/1bQeknRL9vUtkh5s4Fxm7NQ//szH1MTnwcxM0j2Sdrv716c8VJpzMN0xlOk8YFZKnbfJ2c2jbLmi7Hm7GXN2wxaZz97S/78ktUi6192/1JCJzJKZrVblL2dJWijpO2U4BjP7rqRrJS2XdEDSFyT9taTvS1olaa+kj7t7U94gPs38r1XlZQOXtEfSJ6fch9NUzOxqSf8gaaekk9nmz6ly/01ZzsF0x3CzSnIeMDtlztvk7MYoe86Wyp+3mzFn0+EIAAAAyfCGIwAAACRD8QkAAIBkKD4BAACQDMUnAAAAkqH4BAAAQDIUnwAAAEiG4hMAAADJUHwCAAAgmf8Puv5Gc2aVaYIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_example = 18\n",
    "image, label = train_dataset[test_example]\n",
    "\n",
    "model.use_dropout = False\n",
    "\n",
    "prediction = model(image.reshape(1,-1))\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(12,5))\n",
    "\n",
    "ax[0].imshow(image.squeeze(), cmap='gray')\n",
    "ax[1].imshow(image.squeeze(), cmap='gray')\n",
    "\n",
    "ax[0].set_title(labels_map.get(torch.argmax(label).item()), fontsize=14, y=1.02)\n",
    "ax[1].set_title(labels_map.get(torch.argmax(prediction).item()), fontsize=14, y=1.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRxUBGKyPgpJ"
   },
   "source": [
    "<h3>Exercise</h3>\n",
    "Repeat all the steps above using the original MNIST dataset. It is up to you to decide the architecture of the network and hyperparameters (learning rate, number of epochs and batch size). Can you get over 95% accuracy on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_labels(label):\n",
    "    label_vector = torch.zeros(10)\n",
    "    label_vector[label] = 1\n",
    "    return label_vector\n",
    "\n",
    "train_dataset = datasets.MNIST('./mnist_data', download=True, train=True, transform=transforms.Compose([\n",
    "                                                transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                                transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "                                                ]), target_transform=transform_labels)\n",
    "\n",
    "test_dataset = datasets.MNIST('./mnist_data', download=True, train=False, transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                                              transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "                                                          ]), target_transform=transform_labels)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
