{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction and data transformation. It aims to identify the directions (principal components) in which the data varies the most, and represents the data in a new coordinate system defined by these components.\n",
    "\n",
    "The key idea behind PCA is to find a lower-dimensional representation of the data that captures the maximum amount of variance. It achieves this through an orthogonal projection of the original features into a smaller set of uncorrelated variables, called principal components. The first principal component is the direction that captures the most amount of variance in the data, followed by the second principal component, and so on. The principal components are linear combinations of the original features. \n",
    "\n",
    "Here's a step-by-step overview of how PCA works:\n",
    "- **Standardize the data:** If the features have different scales or units, it is important to standardize the data by subtracting the mean and dividing by the standard deviation. This step ensures that all features are on a similar scale and prevents dominance by features with larger variances.\n",
    "\n",
    "- **Compute the covariance matrix:** The covariance matrix is computed to understand the relationships and dependencies between the different features in the data.\n",
    "\n",
    "- **Perform eigendecomposition:** The covariance matrix is decomposed into its eigenvectors and eigenvalues. The eigenvectors represent the principal components, and the eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "- **Select a number of principal components:** The principal components are ranked based on their corresponding eigenvalues, and the top components capturing the most variance are selected. The number of principal components to retain depends on thedesired level of dimensionality reduction.\n",
    "\n",
    "- **Project the data onto the coordinate system defined by the chosen components:** The original data is transformed by projecting it onto the selected principal components. Each data point is represented by its new coordinates in the principal component space.\n",
    "\n",
    "PCA is a powerful tool for exploratory data analysis, visualization, and feature extraction. It helps to identify patterns, reduce noise, and provide a concise representation of the data. PCA is widely used in various fields, including image processing, signal processing, genetics, finance, and social sciences, where dimensionality reduction and data compression are necessary.\n",
    "\n",
    "<img src=\"./images/projplane.gif\" width=\"400\"/>\n",
    "\n",
    "### General procedure (by hand)\n",
    "\n",
    "Let's start by coding the algorithm by hand. Later on, we'll use Sklearn's PCA and project your data into the two first principal components\n",
    "\n",
    "1. Standardize the data\n",
    "2. Calculate the covariance matrix\n",
    "3. Calculate the eigenvalues and eigenvectors of the covariance matrix\n",
    "4. Select the most significant components (~95%) or the first two in case you want to visualize the projection\n",
    "5. Transform the data\n",
    "\n",
    "For our first example, we'll generate 50 random data points close to the line $y=2x$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fc7033ee6d8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "x = np.random.rand(50)\n",
    "y = 2*x + np.random.rand(50)\n",
    "df = pd.DataFrame(np.array([x,y]).T)\n",
    "df.plot.scatter(x=0,y=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below performs all the needed operations by hand. Can you interpret it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.08049023  0.06146803  1.43483327  0.82595893 -1.72324723  1.4943897\n",
      "  1.78771062 -1.16956771 -0.20043595 -0.22909152 -1.93848639 -1.53362266\n",
      " -0.59058193  0.32510826 -0.34423197 -1.01887731 -0.9432649   1.31453505\n",
      "  1.48599331  1.6804994   0.29133354  1.26824076 -2.08813933 -1.7596977\n",
      "  2.0133611   1.26208528  0.18276241  1.21643484  0.57483157  0.16591535\n",
      "  0.47933691 -1.29963633 -0.09509553 -1.49670351 -0.15318394 -1.83411393\n",
      "  0.43134769  1.75052841 -1.74740039 -2.28415306  1.68556698 -0.76506829\n",
      " -2.24397372 -2.0870175  -0.12914932  1.81756985 -1.43365892  1.71836092\n",
      "  2.24484037  1.51489628]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Task 1: In each step, indicate what the code is doing. \n",
    "# Additionally, identify what each of the v1,..,v6 represent and write it in the comments\n",
    "\n",
    "# Step 1: standardize the data by subtracting the mean and dividing by standard deviation. \n",
    "#This is done by each feature individually\n",
    "v1 =(df-df.mean())/df.std()\n",
    "#v1: is the standardized data. \n",
    "\n",
    "# Step 2: compute the covariance matrix \n",
    "v2 = np.cov(v1.T)\n",
    "#v2: is the covariance matrix\n",
    "\n",
    "# Step 3: \n",
    "v3, v4 = np.linalg.eig(v2)\n",
    "#v3:\n",
    "#v4:\n",
    "\n",
    "# Step 4:\n",
    "v5 = 1\n",
    "#v5:\n",
    "\n",
    "# Step 5:\n",
    "v6 = np.dot(v1, v4[:,1:2]).flatten()\n",
    "#v6:\n",
    "\n",
    "# Final Plot\n",
    "plt.scatter(v6, np.zeros_like(v6))\n",
    "print(v6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformation **T = X W** maps a data vector $x_{(i)}$ from an original space with p variables to a lower dimensional space. Nonetheless, we don't need to maintain all principal components. Select the two eigenvectors with the largest eigenvalues and do the follwing mapping: $\\begin{gather*}\n",
    "{\\displaystyle \\mathbf {T}=\\mathbf {X} \\mathbf {W}}\n",
    "\\end{gather*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process, can be automated using the `sklearn` package PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.08049023 -0.06146803 -1.43483327 -0.82595893  1.72324723 -1.4943897\n",
      " -1.78771062  1.16956771  0.20043595  0.22909152  1.93848639  1.53362266\n",
      "  0.59058193 -0.32510826  0.34423197  1.01887731  0.9432649  -1.31453505\n",
      " -1.48599331 -1.6804994  -0.29133354 -1.26824076  2.08813933  1.7596977\n",
      " -2.0133611  -1.26208528 -0.18276241 -1.21643484 -0.57483157 -0.16591535\n",
      " -0.47933691  1.29963633  0.09509553  1.49670351  0.15318394  1.83411393\n",
      " -0.43134769 -1.75052841  1.74740039  2.28415306 -1.68556698  0.76506829\n",
      "  2.24397372  2.0870175   0.12914932 -1.81756985  1.43365892 -1.71836092\n",
      " -2.24484037 -1.51489628]\n"
     ]
    }
   ],
   "source": [
    "# Task 2: Write your comments here\n",
    "# Modify the code below so that you obtain the same result as what you did by hand above: \n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 1\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "proj_sdata = pca.fit_transform(v1)\n",
    "\n",
    "print(proj_sdata.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "colormap = ListedColormap(['c', 'm', 'y'])\n",
    "scatter = plt.scatter(projected_data[:,0], projected_data[:,1], c=y, cmap=colormap)\n",
    "plt.legend(handles = scatter.legend_elements()[0], labels=list(iris['target_names']))\n",
    "plt.xlabel('PCA component 1', fontsize=14)\n",
    "plt.ylabel('PCA component 2', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "df = pd.read_csv(\"gene_expression.csv\")\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop(\"Label\", axis=1)\n",
    "y = df[\"Label\"]\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Visualize PCA\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette=\"Set2\")\n",
    "plt.title(\"PCA on Gene Expression Data\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Explained variance from PCA\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_)[:20], marker='o')\n",
    "plt.title(\"Explained Variance by PCA Components\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# Visualize t-SNE\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=y, palette=\"Set1\")\n",
    "plt.title(\"t-SNE on Gene Expression Data\")\n",
    "plt.xlabel(\"t-SNE 1\")\n",
    "plt.ylabel(\"t-SNE 2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: UMAP\n",
    "reducer = umap.UMAP(random_state=42)\n",
    "X_umap = reducer.fit_transform(X)\n",
    "\n",
    "# Visualize UMAP\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=X_umap[:, 0], y=X_umap[:, 1], hue=y, palette=\"Set3\")\n",
    "plt.title(\"UMAP on Gene Expression Data\")\n",
    "plt.xlabel(\"UMAP 1\")\n",
    "plt.ylabel(\"UMAP 2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris Classification\n",
    "# ðŸŒ¸ The Iris Dataset\n",
    "\n",
    "The **Iris dataset** is a classic dataset used in statistics and machine learning. It contains measurements of iris flowers from three different species.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“¦ Dataset Overview\n",
    "\n",
    "- **Total samples**: 150\n",
    "- **Classes**: 3 iris species\n",
    "  - *Setosa*\n",
    "  - *Versicolor*\n",
    "  - *Virginica*\n",
    "- **Features**: 4 numeric measurements (in cm)\n",
    "\n",
    "| Feature             | Description                           |\n",
    "|---------------------|---------------------------------------|\n",
    "| `sepal length`      | Length of the sepal (outer part)      |\n",
    "| `sepal width`       | Width of the sepal                    |\n",
    "| `petal length`      | Length of the petal (inner part)      |\n",
    "| `petal width`       | Width of the petal                    |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Example Sample\n",
    "\n",
    "| Sepal Length | Sepal Width | Petal Length | Petal Width | Species     |\n",
    "|--------------|-------------|--------------|-------------|-------------|\n",
    "| 5.1          | 3.5         | 1.4          | 0.2         | Setosa      |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Common Uses\n",
    "\n",
    "The Iris dataset is widely used to:\n",
    "\n",
    "- Practice **classification** (e.g., logistic regression, decision trees, SVMs)\n",
    "- Apply **dimensionality reduction** (e.g., PCA, t-SNE)\n",
    "- Explore **clustering** (e.g., k-means)\n",
    "- Visualize data in 2D and 3D\n",
    "- Test basic machine learning workflows\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“¥ Loading the Dataset in Python\n",
    "\n",
    "Using **scikit-learn**:\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data       # shape: (150, 4)\n",
    "y = data.target     # 0 = Setosa, 1 = Versicolor, 2 = Virginica\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "print(iris.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task \n",
    "#Create a vector X in which each element is a list of 4 features\n",
    "\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
