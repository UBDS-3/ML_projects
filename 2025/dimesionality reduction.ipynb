{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b3eb268-67fa-4813-9c1e-813c19d59492",
   "metadata": {},
   "source": [
    "<h4> Principal Component Analysis (PCA) </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73792932-fb4e-4ae1-bf9e-b79aea0307a1",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction and data transformation. It aims to identify the directions (principal components) in which the data varies the most and represents the data in a new coordinate system defined by these components.\n",
    "\n",
    "The key idea behind PCA is to find a lower-dimensional representation of the data that captures the maximum amount of variance. It achieves this by transforming the original features into a new set of uncorrelated variables, called principal components. The first principal component captures the most significant amount of variance in the data, followed by the second principal component, and so on.\n",
    "\n",
    "Here's a step-by-step overview of how PCA works:\n",
    "\n",
    "Standardize the data: If the features have different scales or units, it is important to standardize the data by subtracting the mean and dividing by the standard deviation. This step ensures that all features are on a similar scale and prevents dominance by features with larger variances.\n",
    "\n",
    "Compute the covariance matrix: The covariance matrix is computed to understand the relationships and dependencies between the different features in the data.\n",
    "\n",
    "Perform eigendecomposition: The covariance matrix is decomposed into its eigenvectors and eigenvalues. The eigenvectors represent the principal components, and the eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "Select the principal components: The principal components are ranked based on their corresponding eigenvalues, and the top components capturing the most variance are selected. The number of principal components to retain depends on the desired level of dimensionality reduction.\n",
    "\n",
    "Project the data onto the new coordinate system: The original data is transformed by projecting it onto the selected principal components. Each data point is represented by its new coordinates in the principal component space.\n",
    "\n",
    "PCA is a powerful tool for exploratory data analysis, visualization, and feature extraction. It helps to identify patterns, rbeduce noise, and provide a concise representation of the data. PCA is widely used in various fields, including image processing, signal processing, genetics, finance, and social sciences, where dimensionality reduction and data compression are necessary.\n",
    "\n",
    "*Generated with ChatGTPv3.5 May 24th Version, prompt: \"what is principal component analysis.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64c6a37-36b5-4feb-9b3f-f121ea60051d",
   "metadata": {},
   "source": [
    "![PCA](PCA.png \"Principal Component ANalysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d8f6b7-9141-47c0-99ab-1a17a2eab993",
   "metadata": {},
   "source": [
    "### General procedure\n",
    "\n",
    "1. Standardize the data (features X)\n",
    "2. Calculate the covariance matrix\n",
    "3. Calculate the eigenvalues and eigenvectors of the covariance matrix\n",
    "4. Select the most significant components (~95%) or the first two in case you want to visualize the projection\n",
    "5. Transform the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f10bd90-aea3-4836-a700-83064fbb1714",
   "metadata": {},
   "source": [
    "The transformation **T = X W** maps a data vector $x_{(i)}$ from an original space with p variables to a lower dimensional space. Nonetheless, we don't need to maintain all principal components. Select the two eigenvectors with the largest eigenvalues and do the follwing mapping: $\\begin{gather*}\n",
    "{\\displaystyle \\mathbf {T}=\\mathbf {X} \\mathbf {W}}\n",
    "\\end{gather*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23315ee-e4c3-4479-9ac7-902d1ae27f97",
   "metadata": {},
   "source": [
    "Use Sklearn's PCA and project your data into the two first principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264912a9-ec45-4783-a366-430d25ba946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1 - We standardize the data\n",
    "data = scaler.fit_transform(X)\n",
    "\n",
    "# 2 - We build the covariance matrix\n",
    "# In this case we need features as rows and observations as columns\n",
    "cov_matrix = np.cov(data.T)\n",
    "\n",
    "# 3 - We calculate the eigenvalues and eigenvectors\n",
    "eigenvals, eigenvecs = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# 4 - We select the number of principal components that we want to use for the projection\n",
    "n_components = 2\n",
    "\n",
    "# 5 - We project the data\n",
    "projected_data = np.dot(data, eigenvecs[:,:n_components])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214a4ba5-22a6-4788-b1f7-774419543cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "colormap = ListedColormap(['c', 'm', 'y'])\n",
    "scatter = plt.scatter(projected_data[:,0], projected_data[:,1], c=y, cmap=colormap)\n",
    "plt.legend(handles = scatter.legend_elements()[0], labels=list(iris['target_names']))\n",
    "plt.xlabel('PCA component 1', fontsize=14)\n",
    "plt.ylabel('PCA component 2', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e29c3f0-d235-4369-845e-091f95f8f379",
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance_ratio = eigenvals / np.sum(eigenvals)\n",
    "explained_variance_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568b6919-4569-4198-9fa0-9c149b5c7c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "proj_data = pca.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76757ea5-17f4-48e9-b765-210a409b0be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff8bb65-6550-4a7e-86a2-55222d5372c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvecs[:,:n_components]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a187a9ec-a5f3-4598-b72c-3f75ded1750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_.T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
